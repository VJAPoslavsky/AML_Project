{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AML -- CNN (TPE).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYdi32_ixNJY",
        "outputId": "2ba44367-3626-429f-f990-f30dc4ba4d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install hyperas\n",
        "!pip install hyperopt"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hyperas in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (from hyperas) (0.1.2)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from hyperas) (5.1.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from hyperas) (5.6.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from hyperas) (2.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (4.62.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (3.12.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (1.19.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (2.6.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas) (1.15.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas) (5.3.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas) (5.2.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas) (7.6.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas) (5.2.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->hyperas) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->hyperas) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->hyperas) (5.3.5)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (2.6.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.2.5)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->hyperas) (1.0.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->hyperas) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->hyperas) (3.5.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->hyperas) (4.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->hyperas) (2.11.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->hyperas) (0.12.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->hyperas) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->hyperas) (22.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->hyperas) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->hyperas) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->hyperas) (2.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas) (4.1.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas) (0.5.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->hyperas) (21.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->hyperas) (3.0.6)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->hyperas) (1.11.2)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt) (4.62.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt) (3.12.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt) (2.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10"
      ],
      "metadata": {
        "id": "lSs4BUIUtZ2u"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3rEszUh5TuI"
      },
      "source": [
        "from __future__ import print_function\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, Input\n",
        "from hyperopt import fmin, tpe, hp, anneal, Trials, space_eval, STATUS_OK"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data():# Load the data\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "    img_rows, img_cols = 28, 28\n",
        "    num_classes = 10\n",
        "    channel = 1\n",
        "    #(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "    #img_rows, img_cols = 32, 32\n",
        "    #num_classes = 10\n",
        "    #channel = 3\n",
        "\n",
        "    # Add the channel dimension to the images\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x_train = x_train.reshape(x_train.shape[0], channel, img_rows, img_cols)\n",
        "        x_test = x_test.reshape(x_test.shape[0], channel, img_rows, img_cols)\n",
        "        input_shape = (channel, img_rows, img_cols)\n",
        "    else:\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channel)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channel)\n",
        "        input_shape = (img_rows, img_cols, channel)\n",
        "\n",
        "    print(input_shape)\n",
        "\n",
        "    # Normalize the pixel values to the range of [0, 1]\n",
        "    X_train = x_train.astype('float32') / 255\n",
        "    X_test = x_test.astype('float32') / 255\n",
        "\n",
        "    # convert class vectors to binary class matrices\n",
        "    Y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "    Y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test"
      ],
      "metadata": {
        "id": "gZ8ywmNlz7nD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5rj0HUUm5C_H"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test):\n",
        "    #hp_layers = random.choice([1,2])\n",
        "    #hp_dropout = random.choice([0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
        "    #hp_lr = random.choice(np.random.uniform(0.0001,0.005, 100000))\n",
        "    #hp_filters1 = np.random.choice([*range(32,128+1, 32)])\n",
        "\n",
        "    hp_layers = {{choice([1,2])}}\n",
        "    hp_lr = {{choice(np.arange(0.0001,0.005, 0.0001))}}\n",
        "    hp_filters1 = {{choice([*range(32,128+1, 32)])}}\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    model.add(Conv2D(filters=hp_filters1, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    for i in range(hp_layers):\n",
        "        hp_filters2 = {{choice([*range(32,128+1, 32)])}}\n",
        "        model.add(Conv2D(filters=hp_filters2, kernel_size=3, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D((2,2)))\n",
        "        model.add(Dropout({{choice([0, 0.1, 0.2, 0.3, 0.4, 0.5])}}))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}},\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size={{choice([64, 128])}},\n",
        "              epochs=1,\n",
        "              verbose=2,\n",
        "              validation_data=(X_test, Y_test))\n",
        "    score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Test accuracy:', acc)\n",
        "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
      ],
      "metadata": {
        "id": "oh57ogLB1SMu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjlatkWq5Wab"
      },
      "source": [
        "# See: https://stackoverflow.com/questions/49920031/get-the-path-of-the-notebook-on-google-colab\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Copy/download the file\n",
        "fid = drive.ListFile({'q':\"title='AML -- CNN (TPE).ipynb'\"}).GetList()[0]['id']\n",
        "f = drive.CreateFile({'id': fid})\n",
        "f.GetContentFile('AML -- CNN (TPE).ipynb')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdo3mScvBHF4",
        "outputId": "def2d07d-40f4-4fab-a143-165402ef4098",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "best_run, best_model = optim.minimize(model=model,\n",
        "                                          data=data,\n",
        "                                          max_evals=1,\n",
        "                                          algo=tpe.suggest,\n",
        "                                          notebook_name='AML -- CNN (TPE)', # This is important!\n",
        "                                          trials=Trials())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "from __future__ import print_function\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers.core import Dense, Dropout, Activation\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.datasets import mnist\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.utils import np_utils\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import tensorflow as tf\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import matplotlib.pyplot as plt\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow import keras\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential, Model\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, Input\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import fmin, tpe, hp, anneal, Trials, space_eval, STATUS_OK\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import random\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from pydrive.auth import GoogleAuth\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from pydrive.drive import GoogleDrive\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from google.colab import auth\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from oauth2client.client import GoogleCredentials\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'hp_layers': hp.choice('hp_layers', [1,2]),\n",
            "        'hp_lr': hp.choice('hp_lr', np.arange(0.0001,0.005, 0.0001)),\n",
            "        'hp_filters1': hp.choice('hp_filters1', [*range(32,128+1, 32)]),\n",
            "        'hp_filters1_1': hp.choice('hp_filters1_1', [*range(32,128+1, 32)]),\n",
            "        'Dropout': hp.choice('Dropout', [0, 0.1, 0.2, 0.3, 0.4, 0.5]),\n",
            "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
            "        'batch_size': hp.choice('batch_size', [64, 128]),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "   1: \n",
            "   2: (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
            "   3: img_rows, img_cols = 28, 28\n",
            "   4: num_classes = 10\n",
            "   5: channel = 1\n",
            "   6: #(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
            "   7: #img_rows, img_cols = 32, 32\n",
            "   8: #num_classes = 10\n",
            "   9: #channel = 3\n",
            "  10: \n",
            "  11: # Add the channel dimension to the images\n",
            "  12: if K.image_data_format() == 'channels_first':\n",
            "  13:     x_train = x_train.reshape(x_train.shape[0], channel, img_rows, img_cols)\n",
            "  14:     x_test = x_test.reshape(x_test.shape[0], channel, img_rows, img_cols)\n",
            "  15:     input_shape = (channel, img_rows, img_cols)\n",
            "  16: else:\n",
            "  17:     x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channel)\n",
            "  18:     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channel)\n",
            "  19:     input_shape = (img_rows, img_cols, channel)\n",
            "  20: \n",
            "  21: print(input_shape)\n",
            "  22: \n",
            "  23: # Normalize the pixel values to the range of [0, 1]\n",
            "  24: X_train = x_train.astype('float32') / 255\n",
            "  25: X_test = x_test.astype('float32') / 255\n",
            "  26: \n",
            "  27: # convert class vectors to binary class matrices\n",
            "  28: Y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
            "  29: Y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
            "  30: \n",
            "  31: \n",
            "  32: \n",
            "  33: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3:     #hp_layers = random.choice([1,2])\n",
            "   4:     #hp_dropout = random.choice([0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
            "   5:     #hp_lr = random.choice(np.random.uniform(0.0001,0.005, 100000))\n",
            "   6:     #hp_filters1 = np.random.choice([*range(32,128+1, 32)])\n",
            "   7: \n",
            "   8:     hp_layers = space['hp_layers']\n",
            "   9:     hp_lr = space['hp_lr']\n",
            "  10:     hp_filters1 = space['hp_filters1']\n",
            "  11:     \n",
            "  12:     model = Sequential()\n",
            "  13: \n",
            "  14:     # Input layer\n",
            "  15:     model.add(Conv2D(filters=hp_filters1, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
            "  16:     model.add(BatchNormalization())\n",
            "  17: \n",
            "  18:     for i in range(hp_layers):\n",
            "  19:         hp_filters2 = space['hp_filters1_1']\n",
            "  20:         model.add(Conv2D(filters=hp_filters2, kernel_size=3, activation='relu'))\n",
            "  21:         model.add(BatchNormalization())\n",
            "  22:         model.add(MaxPooling2D((2,2)))\n",
            "  23:         model.add(Dropout(space['Dropout']))\n",
            "  24:     \n",
            "  25:     model.add(Flatten())\n",
            "  26:     model.add(Dense(10, activation='softmax'))\n",
            "  27: \n",
            "  28:     model.compile(loss='categorical_crossentropy',\n",
            "  29:                   optimizer=space['optimizer'],\n",
            "  30:                   metrics=['accuracy'])\n",
            "  31: \n",
            "  32:     model.fit(X_train, Y_train,\n",
            "  33:               batch_size=space['batch_size'],\n",
            "  34:               epochs=1,\n",
            "  35:               verbose=2,\n",
            "  36:               validation_data=(X_test, Y_test))\n",
            "  37:     score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
            "  38:     print('Test accuracy:', acc)\n",
            "  39:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
            "  40: \n",
            "(28, 28, 1)\n",
            "469/469 - 20s - loss: 0.3054 - accuracy: 0.9288 - val_loss: 0.5071 - val_accuracy: 0.8151 - 20s/epoch - 42ms/step\n",
            "\n",
            "Test accuracy:\n",
            "0.8151000142097473\n",
            "100%|██████████| 1/1 [00:22<00:00, 22.16s/it, best loss: -0.8151000142097473]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test = data()\n",
        "history = best_model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test), verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiLnhJcc2zYI",
        "outputId": "3fae7f4b-738a-45e3-cc40-233f73545a24"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28, 1)\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.1972 - accuracy: 0.9588 - val_loss: 0.1060 - val_accuracy: 0.9781\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1404 - accuracy: 0.9731 - val_loss: 0.0882 - val_accuracy: 0.9827\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1127 - accuracy: 0.9781 - val_loss: 0.1321 - val_accuracy: 0.9763\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0882 - accuracy: 0.9818 - val_loss: 0.0814 - val_accuracy: 0.9844\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0843 - accuracy: 0.9831 - val_loss: 0.0856 - val_accuracy: 0.9827\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0644 - accuracy: 0.9865 - val_loss: 0.0638 - val_accuracy: 0.9871\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0634 - accuracy: 0.9859 - val_loss: 0.0627 - val_accuracy: 0.9883\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0535 - accuracy: 0.9877 - val_loss: 0.0840 - val_accuracy: 0.9845\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0509 - accuracy: 0.9881 - val_loss: 0.0722 - val_accuracy: 0.9878\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0458 - accuracy: 0.9893 - val_loss: 0.0749 - val_accuracy: 0.9874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaEviY3SbLlB"
      },
      "source": [
        "TPE_accuracy = history.history['val_accuracy']\n",
        "TPE_loss = history.history['val_loss']\n",
        "TPE_training_accuracy = history.history['accuracy']\n",
        "TPE_training_loss = history.history['loss']\n",
        "\n",
        "np.save('TPE_MNIST.npy', TPE_accuracy) # save, change based on what dataset is used"
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}